# -*- coding: utf-8 -*-
"""Neural_NetworkFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qfm0vRCaVqFPq9ZX1flsW2sSJ6vx5bbb
"""

import numpy as np
import random 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import math
import csv
import seaborn as sns
# from fitter import Fitter, get_common_distributions, get_distributions

def normalize(data):
    result = data.copy()
    
    for col in data.columns:
        max_value = data[col].max()
        min_value = data[col].min()
        result[col] = (data[col] - min_value) / (max_value - min_value)
    return result



"""### **Data Preprocessing**"""

#@title Normilization
#reading data from data file
dataRaw= pd.read_csv("ce889_dataCollection_prev.csv",index_col=False)
#normilizing the data into the range of 0 to 1
normData=normalize(dataRaw)
#converting it into list
inputsNorm = normData.iloc[:, 0:2]
outputsNorm=normData.iloc[:, 2:4]
print(normData)
# normData.to_csv("file.csv", sep=',')

#checking null 0,or na values
dataRaw.isna().sum()

dataRaw.info()
# da=inputsNorm.pop(0)
# print(da)

sns.set_style('white')
sns.set_context("paper", font_scale = 2)
sns.displot(data=dataRaw, x=dataRaw['200.4266105'], kind="hist", bins = 100, aspect = 1.5)
print("input 1")

sns.set_style('white')
sns.set_context("paper", font_scale = 2)
sns.displot(data=dataRaw, x=dataRaw['307.1'], kind="hist", bins = 100, aspect = 1.5)
print("input 2")

sns.set_style('white')
sns.set_context("paper", font_scale = 2)
sns.displot(data=dataRaw, x="0", kind="hist", bins = 100, aspect = 1.5)
print("output 1")

sns.set_style('white')
sns.set_context("paper", font_scale = 2)
sns.displot(data=dataRaw, x="0.1", kind="hist", bins = 100, aspect = 1.5)
print("output 2")

#@title Splitting data
#splitting the data with shuffle and random state
X_train, X_test, y_train, y_test = train_test_split(inputsNorm, outputsNorm,test_size=0.3, shuffle = True, random_state = 35)
inputsNormlist=X_train.values.tolist()
outputNormlist=y_train.values.tolist()
# print(inputsNormlist)

#@title Splitting data testing validation
#splitting the data with shuffle and random state
Xtest, XVal, Ytest, YVal = train_test_split(X_test, y_test,test_size=0.15, shuffle = True, random_state = 35)

"""### **Neural Network Class**"""



class NeuralNetwork():
    def __init__(self,numberOfNeurons):
      #initializing neural network values
        self.inputValue=None
        self.numberOfNeurons=numberOfNeurons
        self.currentweights = None
        self.previousWeights=None
        self.lmbda = 0.6
        self.eitha=0.9
        self.alpha=0.01
        self.deltaWeight=None
        self.deltaWeightPrev=None
        self.localGradientz=None
        
    #generate random  weights
    def generateWeight(self,rows,cols):
        weights=[]
        weights=np.random.random((rows,cols))
        biasWeight=np.random.random(cols)
        return weights,biasWeight
    #multiplying input into weights with bias
    def InputIntoWeight(self,weights,biasWeight,inputValue):
      #assigning weights 
        self.inputValue=inputValue
        self.currentweights=weights        
        inputWeightss=[]
        biasInput=[]
        biasInput=inputValue.copy()
        # inserting bias input in input value
        biasInput.insert(0,1)
        sums=0       
        #weights into input  
        for i in range(len(self.currentweights[1])):
            for j in range(len(inputValue)):
                sums+=inputValue[j]*self.currentweights[j][i]
            
            inputWeightss.append(sums)
            sums=0
        return inputWeightss,biasInput

    #activation function sigmoid
    def sig(self,x):
      #calculating sigmoid values
        sigmoidValues=[]
        for i in range(len(x)):
            sigmoidValues.append(round(1/(1 + np.exp(-(self.lmbda*x[i]))),6))
        return sigmoidValues
    
        
    
   #backward

    #error calculation
    def errorCalculation(self,output,desiredOutput):
        errorUpdate=[]
        for i in range(0,len(output)):
            errorUpdate.append(round((desiredOutput[i]-output[i]),6))
        return errorUpdate
    
    #local gradient for backwards 
    def localGradient(self,output,error,flagHiddenInput,localGradientsPrev,prevWeightsOfOutput):
        localGradients=[]
        sumOfWeight=0
        numcols = prevWeightsOfOutput
#       output to hidden layer check
        if flagHiddenInput==False:
            for i in range(0,len(output)):
                localGradients.append(self.lmbda*output[i]*(1-output[i])*error[i])               
            self.localGradientz=localGradients
#   hidden to input
        else:
            transpos=prevWeightsOfOutput.T
            sumofWeights=0
            lgIntoPrevWeights=[]
            #mulitplying previous weights with previous layer local gradients
            for i in range(len(prevWeightsOfOutput)):
                for j in range(len(localGradientsPrev)):
                    sumofWeights+=localGradientsPrev[j]*prevWeightsOfOutput[i][j]
                lgIntoPrevWeights.append(sumofWeights)
                sumofWeights=0
            #calculating local gradient 
            for i in range(len(output)):
                localGradients.append(self.lmbda*output[i]*(1-output[i])*lgIntoPrevWeights[i])
            self.localGradientz=localGradients   
        return localGradients
    
    
    #calculating delta weights
    def CalculatingDeltaWeight(self,localGradientLayer,activationLayer1,prevDeltaweight,firstIterate,alpha):
        numrows = len(self.currentweights)
        numcols = len(self.currentweights[1])
        deltaWeights=[]
        sumOfPrevAlpha=[]
        multiplyAlpha=0
    #delta weights for first iteration
        if firstIterate==True:
            for i in range(len(localGradientLayer)):
                    for j in range(len(activationLayer1)):
                        deltaWeights.append(self.eitha*localGradientLayer[i]*activationLayer1[j])
            # print("delta weights",deltaWeights)
            self.deltaWeight=deltaWeights
    #delta weights for second iteration with alpha 
        else:
            prevDeltaweight=np.array(prevDeltaweight).reshape(numrows, numcols)
            for i in range(len(prevDeltaweight)):
                for j in range(len(prevDeltaweight[1])):
                    sumOfPrevAlpha.append(alpha*prevDeltaweight[i][j])
            
            sumOfPrevAlphaArray =np.array(sumOfPrevAlpha).reshape(numrows, numcols)
            for i in range(len(localGradientLayer)):
                    for j in range(len(activationLayer1)):
                        deltaWeights.append(self.eitha*localGradientLayer[i]*activationLayer1[j]+sumOfPrevAlphaArray[j][i])
        self.deltaWeightPrev=self.deltaWeight
        self.deltaWeight=deltaWeights          
        return self.deltaWeight
    
    #updating the weights by adding delta weights and previous weights 
    def updateWeights(self,deltaWeights,biasweights,biasweightsLG,activationValues,output,lenactivationValues,hiddentoInput,lenInput):
        newWeights=[]
        biasWeights=[]
        rows=len(self.currentweights)
        cols=len(self.currentweights[1])
        
        biasWeights.append(biasweights+biasweightsLG)
        #reshaping delatweights and transposing
        deltaWeightsArray=np.array(deltaWeights).reshape(cols, rows)
        a=deltaWeightsArray.T

        # adding delta weights with current weights to get new weights
        newWeights=a+self.currentweights    
        self.previousWeights=self.currentweights
        self.currentweights=newWeights

        return self.currentweights,biasWeights

"""### Initializing Neural Network Architechture"""

numberOfNeurons=6
desiredOutput=outputNormlist.pop(0)
print(desiredOutput)
inputValue=inputsNormlist.pop(0)
print(inputValue)

#initializing neural network
inputToHiddenLayer=NeuralNetwork(numberOfNeurons)
hiddenToOutputLayer=NeuralNetwork(numberOfNeurons)
# initializng weights
mse=[]
weightsInputHidden,biasWeightInput=inputToHiddenLayer.generateWeight(len(inputValue),numberOfNeurons)
weightsHiddenOutput,biasWeightOutput=hiddenToOutputLayer.generateWeight(numberOfNeurons,len(desiredOutput))

#feed forward first iterate
  # input to hidden
inputWeights,biasInput1=inputToHiddenLayer.InputIntoWeight(weightsInputHidden,biasWeightInput,inputValue)
activationHidden=inputToHiddenLayer.sig(inputWeights)
    #hidden to output
hiddenWeights,biasHidden1=hiddenToOutputLayer.InputIntoWeight(weightsHiddenOutput,biasWeightOutput,activationHidden)
output=hiddenToOutputLayer.sig(hiddenWeights)
print("output",output)

#initializing lists
errorList=[]
outputError1=[]
outputError2=[]
avgError=[]
mseavg=[]
msey1=[]
msey2=[]
#backward
#error calculation 
error=hiddenToOutputLayer.errorCalculation(output,desiredOutput)
errorList.append(error)
    #calculating output to hidden
localGradients=hiddenToOutputLayer.localGradient(output,error,False,0,0)
deltaWeightsoutput=hiddenToOutputLayer.CalculatingDeltaWeight(localGradients,activationHidden,0,True,0.01)
hiddenToOutputLayer.updateWeights(deltaWeightsoutput,biasHidden1,biasHidden1,activationHidden,output,activationHidden,False,0)
    #calculating hidden to input
localGradientshidden=inputToHiddenLayer.localGradient(activationHidden,0,True,localGradients,hiddenToOutputLayer.previousWeights)
deltaWeightsHidden=inputToHiddenLayer.CalculatingDeltaWeight(localGradientshidden,inputValue,0,True,0.01)
weightsHiddenInput=inputToHiddenLayer.updateWeights(deltaWeightsHidden,biasInput1,biasInput1,activationHidden,output,activationHidden,True,inputValue)

print(inputToHiddenLayer.currentweights)
print(hiddenToOutputLayer.currentweights)

def meanSquareError(error):
    sumMse=0
    for i in range(len(error)):
        sumMse+=(error[i]**2)
    mse=(1/len(error))*sumMse
    return mse

#training 
for i in range(180):
    for j in range(len(inputsNormlist)):
        #feedforward
#         input to hidden
        wxInputHidden,biasInput=inputToHiddenLayer.InputIntoWeight(inputToHiddenLayer.currentweights,biasWeightInput,inputsNormlist[j])
        activationHiddenLayer=inputToHiddenLayer.sig(wxInputHidden)
        
        #hidden to output
        wxHiddenOutput,biasHidden=hiddenToOutputLayer.InputIntoWeight(hiddenToOutputLayer.currentweights,biasWeightOutput,activationHiddenLayer)
        outputPredicted=hiddenToOutputLayer.sig(wxHiddenOutput)

        #error
        error=hiddenToOutputLayer.errorCalculation(outputPredicted,outputNormlist[j])
        errorList.append(error)
        avgError.append((sum(error))/2)
        outputError1.append(error[0])
        outputError2.append(error[1])
        
        #backward
#         output to hidden
        LGOutputHidden=hiddenToOutputLayer.localGradient(outputPredicted,error,False,0,0)
        deltaWeightsoutput=hiddenToOutputLayer.CalculatingDeltaWeight(LGOutputHidden,activationHiddenLayer,hiddenToOutputLayer.deltaWeightPrev,False,0.01)
        hiddenToOutputLayer.updateWeights(deltaWeightsoutput,biasHidden1,biasHidden1,activationHiddenLayer,outputPredicted,activationHiddenLayer,False,0)
        
        
        #hidden to input
        LGHiddenInput=inputToHiddenLayer.localGradient(activationHiddenLayer,0,True,LGOutputHidden,hiddenToOutputLayer.previousWeights)
        deltaWeightsHidden=inputToHiddenLayer.CalculatingDeltaWeight(LGHiddenInput,inputsNormlist[j],inputToHiddenLayer.deltaWeightPrev,False,0.01)
        inputToHiddenLayer.updateWeights(deltaWeightsHidden,biasInput1,biasInput1,activationHiddenLayer,0,activationHiddenLayer,True,inputsNormlist[j])
    mseavg.append(math.sqrt(meanSquareError(avgError)))
    msey1.append(math.sqrt(meanSquareError(outputError1)))
    msey2.append(math.sqrt(meanSquareError(outputError2)))

#RMSE training
plt.plot(mseavg)
plt.show()
# print(len(mseavg))

print("RSME 69",mseavg[69])
print("RSME 79",mseavg[79])
print("RSME 89",mseavg[89])
print("RSME 99",mseavg[99])
print("RSME 109",mseavg[109])
print("RSME 119",mseavg[119])
print("RSME 130",mseavg[130])
print("RSME 149",mseavg[149])
print("RSME 159",mseavg[159])
print("RSME 169",mseavg[169])
print("RSME 179",mseavg[179])
# print("RSME 188 ",mseavg[189])
# print("RSME 199 ",mseavg[199])

print("input to hidden")
print(inputToHiddenLayer.currentweights)
print("hidden to output")
print(hiddenToOutputLayer.currentweights)

np.savetxt('weights_inputHidden.txt', inputToHiddenLayer.currentweights, fmt='%f')
np.savetxt('weights_hiddenToOutput.txt', hiddenToOutputLayer.currentweights, fmt='%f')

"""### Validation """

inputsTestlist=X_test.values.tolist()
outputTestlist=y_test.values.tolist()

# Validation
inputsNormlistVal=XVal.values.tolist()
outputNormlistVal=YVal.values.tolist()

mseavgs=[]
avgErrors=[]
def validation(inputsNormlistVal,outputNormlistVal):
  for i in range(150):
      for j in range(len(inputsNormlistVal)):
        # print(inputsNormlistVal[j])
        
          #feedforward
  #         input to hidden
        wxInputHidden,biasInput=inputToHiddenLayer.InputIntoWeight(inputToHiddenLayer.currentweights,biasWeightInput,inputsNormlistVal[j])
        activationHiddenLayer=inputToHiddenLayer.sig(wxInputHidden)
          
          #hidden to output
        wxHiddenOutput,biasHidden=hiddenToOutputLayer.InputIntoWeight(hiddenToOutputLayer.currentweights,biasWeightOutput,activationHiddenLayer)
        outputPredicted=hiddenToOutputLayer.sig(wxHiddenOutput)
          # print("output predicted",outputPredicted)

          #error
        error=hiddenToOutputLayer.errorCalculation(outputPredicted,outputNormlistVal[j])
        errorList.append(error)
        avgErrors.append((sum(error))/2)
        
          #backward
  #         output to hidden
        LGOutputHidden=hiddenToOutputLayer.localGradient(outputPredicted,error,False,0,0)
  #         print(hiddenToOutputLayer.deltaWeightPrev)
        deltaWeightsoutput=hiddenToOutputLayer.CalculatingDeltaWeight(LGOutputHidden,activationHiddenLayer,hiddenToOutputLayer.deltaWeightPrev,False,0.01)
        hiddenToOutputLayer.updateWeights(deltaWeightsoutput,biasHidden1,biasHidden1,activationHiddenLayer,outputPredicted,activationHiddenLayer,False,0)
          
          
          #hidden to input
        LGHiddenInput=inputToHiddenLayer.localGradient(activationHiddenLayer,0,True,LGOutputHidden,hiddenToOutputLayer.previousWeights)
        deltaWeightsHidden=inputToHiddenLayer.CalculatingDeltaWeight(LGHiddenInput,inputsNormlistVal[j],inputToHiddenLayer.deltaWeightPrev,False,0.01)
        inputToHiddenLayer.updateWeights(deltaWeightsHidden,biasInput1,biasInput1,activationHiddenLayer,0,activationHiddenLayer,True,inputsNormlistVal[j])
      mseavgs.append(math.sqrt(meanSquareError(avgErrors)))
      # print(mseavgs)
  return mseavgs

mseavgss=validation(inputsNormlistVal,outputNormlistVal) 
plt.plot(mseavgss)
plt.show()

# weightsInputHidden=inputToHiddenLayer.currentweights
# weightsHiddenOutput=hiddenToOutputLayer.currentweights
print(inputToHiddenLayer.currentweights)
print(hiddenToOutputLayer.currentweights)

"""### Test"""

def InputIntoWeightTest(weights,inputValue):        
  inputWeightss=[]
  sums=0         
  for i in range(len(weights[1])):
    for j in range(len(inputValue)):
      sums+=inputValue[j]*weights[j][i]
    inputWeightss.append(sums)
    # print("input into weights",inputWeightss)
    sums=0
  return inputWeightss
def sigTest(x):
  lmbda=0.8
  sigmoidValues=[]
  for i in range(len(x)):
    sigmoidValues.append(round(1/(1 + np.exp(-(lmbda*x[i]))),6))
  return sigmoidValues

mseTest=[]
avgErrors=[]
inputsNormlistTest=Xtest.values.tolist()
outputNormlistTest=Ytest.values.tolist()
def testing(inputsNormlistTest,outputNormlistTest):
  for j in range(len(inputsNormlistTest)):
    #input to hidden
    wxInputHidden=InputIntoWeightTest(inputToHiddenLayer.currentweights,inputsNormlistTest[j])
    activationHiddenLayer=sigTest(wxInputHidden)
            #hidden to output
    wxHiddenOutput=InputIntoWeightTest(hiddenToOutputLayer.currentweights,activationHiddenLayer)
    #predict
    outputPredicted=sigTest(wxHiddenOutput)
    #error
    error1=output[0]-outputNormlistTest[j][0]
    error2=output[1]-outputNormlistTest[j][1]
    avgErrors.append((error1+error2)/2)
    mseTest.append(math.sqrt(meanSquareError(avgErrors)))
  return mseTest

testing(inputsNormlistTest,outputNormlistTest)

plt.plot(mseTest)
plt.show()
print(len(mseTest))
print(mseTest[2282])

